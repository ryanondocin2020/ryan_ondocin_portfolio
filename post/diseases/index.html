<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Cardiac Disease Prediction Models | Ryan Ondocin</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Naive Bayes Classifiers, K Nearest Neighbors, Support Vector Machines(with both linear and non-linear kernel functions), Random Forest and Gradient Boosting Classifier&#39;s to build a disease diagnosis model">
    <meta name="generator" content="Hugo 0.79.0" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="https://ryanondocin2020.github.io/ryan_ondocin_portfolio/dist/css/app.4fc0b62e4b82c997bb0041217cd6b979.css" rel="stylesheet">
    

    

    
      

    

    
    
    <meta property="og:title" content="Cardiac Disease Prediction Models" />
<meta property="og:description" content="Naive Bayes Classifiers, K Nearest Neighbors, Support Vector Machines(with both linear and non-linear kernel functions), Random Forest and Gradient Boosting Classifier&#39;s to build a disease diagnosis model" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ryanondocin2020.github.io/ryan_ondocin_portfolio/post/diseases/" />
<meta property="article:published_time" content="2019-02-25T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-02-25T00:00:00+00:00" />
<meta itemprop="name" content="Cardiac Disease Prediction Models">
<meta itemprop="description" content="Naive Bayes Classifiers, K Nearest Neighbors, Support Vector Machines(with both linear and non-linear kernel functions), Random Forest and Gradient Boosting Classifier&#39;s to build a disease diagnosis model">
<meta itemprop="datePublished" content="2019-02-25T00:00:00+00:00" />
<meta itemprop="dateModified" content="2019-02-25T00:00:00+00:00" />
<meta itemprop="wordCount" content="3482">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Cardiac Disease Prediction Models"/>
<meta name="twitter:description" content="Naive Bayes Classifiers, K Nearest Neighbors, Support Vector Machines(with both linear and non-linear kernel functions), Random Forest and Gradient Boosting Classifier&#39;s to build a disease diagnosis model"/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fopengovasia.com%2Fwp-content%2Fuploads%2F2018%2F09%2F59e6efb16032750001877f97_NTU-NNI-machine-learning-artificial-intelligence-neuroscience.jpg&amp;f=1&amp;nofb=1');">
    <div class="pb3-m pb6-l bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://ryanondocin2020.github.io/ryan_ondocin_portfolio/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Ryan Ondocin
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://ryanondocin2020.github.io/ryan_ondocin_portfolio/about/" title="About page">
              About
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://ryanondocin2020.github.io/ryan_ondocin_portfolio/contact/" title="Experience page">
              Experience
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://ryanondocin2020.github.io/ryan_ondocin_portfolio/post/" title="Projects page">
              Projects
            </a>
          </li>
          
        </ul>
      
      







<a href="https://www.linkedin.com/in/ryan-ondocin-5699b7138/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/Ryanondocin2019?tab=repositories" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>








    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Cardiac Disease Prediction Models</h1>
          
            <h2 class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
              Naive Bayes Classifiers, K Nearest Neighbors, Support Vector Machines(with both linear and non-linear kernel functions), Random Forest and Gradient Boosting Classifier&#39;s to build a disease diagnosis model
            </h2>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph6">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        PROJECTS
      </aside>
      




  <div id="sharing" class="mt3">



      <h1 class="f1 athelas mt3 mb1">Cardiac Disease Prediction Models</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2019-02-25T00:00:00Z">February 25, 2019</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-three-thirds-l"><h1 id="brief-overview">Brief Overview:</h1>
<p>This project deals with Naive Bayes Classifiers, K Nearest Neighbors, Support Vector Machines(with both linear and non-linear kernel functions), Random Forest and Gradient Boosting Classifier&rsquo;s to build a disease diagnosis model. It is a binary classification problem to predict whether or not a patient has a certain unspecified disease.</p>
<h3 id="training-data-attributes">Training Data Attributes:</h3>
<ul>
<li>Age: in years</li>
<li>Gender: male/female</li>
<li>Height: in unit of cm</li>
<li>Weight: in unit of kg</li>
<li>Low Blood Pressure: lower bound of blood pressure measurement</li>
<li>High Blood Pressure: higher bound of blood pressure measurement</li>
<li>Cholesterol: three cholesteral levels</li>
<li>Glucose: three glucose levels</li>
<li>Smoke: 1/0 regarding if the patient smokes</li>
<li>Alcohol: 1/0 regarding if the patient drinks alcohol</li>
<li>Exercise: 1/0 regarding if the patient exercises regularly</li>
<li>Disease: The binary target variable. Does the patient have the disease?</li>
</ul>
<p>Due to these parameters, I believe that our target variable will have something to do with Coronary heart disease. According to the CDC, heart disease is the leading cause of death in the US and affects ~47% of Americans who have one of the following conditions: Diabetes, Overweight and obesity, Unhealthy diet, Physical inactivity, or Excessive alcohol usage. Based on this, it is apparent that our collected data is related to each of the aforementioned risk factors. We will try to build several predicition models that can accurately classify this disease and we will use these tuned algorithms to predict on a testing dataset.</p>
<h3 id="master-performance-table">Master Performance Table</h3>
<figure>
    <img src="https://i.ibb.co/fSCyGjY/newplot-6.png"/> 
</figure>

<h4 id="in-summation-this-exercise-executes-the-following">In Summation, this exercise executes the following</h4>
<ul>
<li>Implemented machine learning algorithms such as Decision Trees, Support Vector Machines (SVM), Random Forest, Naïve Bayes, K-Nearest Neighbours (KNN), Logistic Regression, and Artificial Neural Networks.</li>
<li>Used Grid Search and k-fold cross-validation to tune the hyper-parameters in order to optimize the recall and accuracy of the models</li>
<li>Created disease predictions with accuracy and recall values of around 78%</li>
<li>Performed exploratory data analysis in Python on a medical data set consisting of 50,000 rows, using statistical analysis and visualization</li>
</ul>
<p><figure>
    <img src="https://i.ibb.co/9VKtJ2D/Screen-Shot-2020-12-18-at-1-00-28-PM.png"/> 
</figure>
<br />
<figure>
    <img src="https://i.ibb.co/zQ5h4tN/Screen-Shot-2020-12-18-at-1-02-05-PM.png"/> 
</figure>
<br />
<figure>
    <img src="https://i.ibb.co/93ytwc4/Screen-Shot-2020-12-18-at-1-02-32-PM.png"/> 
</figure>
<br />
<figure>
    <img src="https://i.ibb.co/XCQvGR8/Screen-Shot-2020-12-18-at-1-02-58-PM.png"/> 
</figure>
<br />
<figure>
    <img src="https://i.ibb.co/JxTPKrL/Screen-Shot-2020-12-18-at-1-06-31-PM.png"/> 
</figure>
</p>
<h2 id="correlation-heatmap-of-features">Correlation Heatmap of Features</h2>
<h2 id="data-quality-issuesinitial-thoughts">Data Quality Issues/Initial thoughts:</h2>
<h4 id="its-important-to-obtain-some-more-contextual-information-about-the-patient-first">It&rsquo;s important to obtain some more contextual information about the patient first.</h4>
<ul>
<li>
<p>Firstly, the data collection step is extremely flawed. Scoring things such as &ldquo;Smoke&rdquo;/&ldquo;Alcohol&rdquo;/&ldquo;Exercise&rdquo; on a binary scale is a difficult measure to guage.</p>
</li>
<li>
<p>How often do you smoke, 1 cigarette/week, or 1 pack/day? The same thing could be said for Alcohol and Exercise.</p>
</li>
<li>
<p>If the patient was simply asked these question on the spot, they may have a good reason to be dishonest. Their insurance premiums may rise if they answer yes.</p>
</li>
<li>
<p>Alcoholism, for example could lead to liver Psoriasis which can be inferred from things such as excess Bilirupin build-up in the blood. Blood percolates through the liver, so if theres damage to it you get a high venous blood pressure which is called portal hypertension.</p>
</li>
<li>
<p>Ironically, alcohol raises HDL (good cholesterol), so if people only drink and don&rsquo;t smoke they rarely tend to get coronary disease, but unfortunately they may die of liver failure. This would make these discrete numerical attributes much more reliable in our model.</p>
</li>
<li>
<p>If a patients exercises 6 times/week for two hours/day, then they should have really good blood circulation but their systolic blood pressure is a bit high which means they are exerting too much energy(than for what would be considered healthy). If a patient considers moving at all as sufficient exercise than we&rsquo;re going to see an inconsistency in the results.</p>
</li>
<li>
<p>If a patient has other conditions such as diabetes, then their HbA1c (glucose levels) may be in an unhealthy range which could increase the chance of atherosclerosis thus increasing the chance of coronary disease. You can see how quickly the data collection process can become too expensive/not feasible. There are thousands of other factors/connditions(hereditary/habitual) that could be excellent predictors of cardiac disease but what if we also would like to quantify the type or severity of the disease. Heart disease is a blanket term and it could be used to refer to either the arteries (blockage/obstruction) or the heart in general. Arteriosclerosis occurs when the epi-cardial arteries can&rsquo;t provide enough enough oxygen to the heart. Monitor their LDL/HDL/Triglycerides/Cholesterol.</p>
</li>
</ul>
<h3 id="other-questions-that-could-help">Other questions that could help</h3>
<ul>
<li>Has anyone in their family died/been diagnosed with this disease?</li>
<li>What medications do they currently take? How often?</li>
<li>Most of the patients are older I notice, so are the younger patients(30&rsquo;s) there because of cardiac arrests/early signs?</li>
<li>For now, I&rsquo;m assuming that if the patient&rsquo;s Glucose level was measured then they probably have diabetes(or a family history) because it only accounts for the past 3 months and needs to be constantly monitored.</li>
<li>If the patient doesn&rsquo;t have diabetes nor come in for regular check-ups, are their Glucose levels automatically assigned to &lsquo;normal&rsquo;?</li>
<li>The same thing can be said for Glucose. Older people are obviously at higher risk for Coronary disease than younger people, so you would like the model to be representative of the population on which it was trained.</li>
<li>If we feed our model copious amounts of data from adolescents/teenagers/young adults, then I would expect it to perform much worse.</li>
<li>Older people come in for annnual/bi-annual check-ups with their primary care provider more often so we might have to assume that cholesterol and glucose are accurately represented throughout our training/testing data.</li>
</ul>
<h1 id="data-cleaning">Data Cleaning:</h1>
<ul>
<li>I&rsquo;ve decided to clip noisy values(HBP/LBP) at their upper and lower fence limits(+1/-1,respectively) according to their distributions.</li>
<li>I&rsquo;ve also decided to clip the lower limits on height and weight due to their distributions. (98% and 1% percentile)</li>
<li>These clipped values were then replaced with nan values and imputed using MICE.</li>
<li>Performed feature engineering by filling in statistical outliers with NA values and replacing them using MICE imputation techniques</li>
</ul>
<h1 id="filling-remaining-numerical-nas-with-mice-imputation">Filling remaining numerical NAs with MICE imputation:</h1>
<p>I&rsquo;ve used multiple imputations to account for the uncertainty association  between each imputation if I were to use KNearest Neighbors. Single imputation doesn&rsquo;t acount for that so MICE will help us find the most likely value overall.</p>
<p>MICE essentially uses multiple regression to map the distributions of each value in order to accurately impute what is missing. I&rsquo;ve found that this method is very flexible and much less time consuming than if I were to use KNearest Neighbors.</p>
<p>Although Low/High BP constitute a large portion of outliers in our data set, they do show a strong correlation to disease. I initially tried dropping outliers from our models, however recall was notably increased after I decided to use this imputation measure instead</p>
<figure>
    <img src="https://i.ibb.co/yyk3XHC/Screen-Shot-2020-12-19-at-1-28-30-AM.png"/> <figcaption>
            <h4>Box plots of attributes following MICE Imputation</h4>
        </figcaption>
</figure>

<h1 id="modeling">Modeling:</h1>
<h4 id="note-the-goal-for-every-model-is-to-maximize-recall">NOTE: The goal for every model is to maximize recall.</h4>
<p>We would like to maximize recall in these models because it is better to predict that a patient has the disease when he/she doesn&rsquo;t rather than predict no disease when they do. This could lead to people missing the critical time window for treatment to occur and should be considered a life or death situation.<br />
That being said, we don&rsquo;t want to compromise all accuracy just so recall is the highest (could only predict positives for example). We will carry on with this idea in mindn</p>
<h1 id="naive-bayes-classifier">Naive Bayes Classifier:</h1>
<p>Supervised, probabilistic learning method for classification<br />
Given a set of attribute/feature values, a Naive Bayes Classifier (NBC) predicts a distribution over a set of outcomes<br />
The final probability will be rounded to the nearest whole value in order to make a classification.</p>
<h5 id="called-naive-because-we-assume-independence-between-each-attribute-this-lets-us-reduce-complexity-exponential-to-a-linear-for-the---of-possibilities-for-each-features-values-estimate-2--k--attributes">Called Naive because we assume independence between each attribute. This lets us reduce complexity (exponential to a linear) for the  # of possibilities for each features values. Estimate 2 ^ k (# attributes).</h5>
<ul>
<li>Gaussian Naive Bayes Classifer: assumes that each attribute is normally distributed/continuous<br />
<figure>
    <img src="https://i.ibb.co/f2v13jm/Screen-Shot-2020-12-19-at-12-50-26-AM.png"/> <figcaption>
            <h4>Gaussian Naive Bayes Classifier ROC AUC</h4>
        </figcaption>
</figure>
<br />
<figure>
    <img src="https://i.ibb.co/LpbX76v/Screen-Shot-2020-12-19-at-12-52-43-AM.png"/> <figcaption>
            <h4>Gaussian Naive Bayes Classifier Model Metrics</h4>
        </figcaption>
</figure>
</li>
</ul>
<h1 id="knn-model">KNN model:</h1>
<p>Nearest Neighbor Classification is a flexible, lazy learning technique that that finds all training examples that are relatively similar to the testing attributes.</p>
<ul>
<li>if K is too small, we are susceptible for overfitting because of increased variance (noisy outliers)</li>
<li>if K is too large the classifier may include data points that are too far away from its neighborhood which could lead to misclassification</li>
<li>It&rsquo;s a distance based algorithm that computes the similarity between each test example and all training examples.</li>
</ul>
<p>Distance based so we must scale the data down so each observation is in proximity (helps create appropriate neighborhoods)<br />
Attributes with differing scales(Blood Pressure/Height/Weight) need to have similar variance in relation to one another. I&rsquo;ve chosen MinMaxscaler to squash all values between 0 and 1. After extensive testing this has led to the best results on all of my models.</p>
<h2 id="kmeans-error-rate">KMeans Error Rate</h2>
<figure>
    <img src="https://i.ibb.co/CM9TY23/Screen-Shot-2020-12-19-at-12-54-35-AM.png"/> 
</figure>

<h4 id="knn---creates-arbitrarily-shaped-decision-boundaries-which-is-more-flexible-than-classifiers-constrained-to-rectilinear-decision-boundaries-dtlinearsvc-">KNN - creates arbitrarily shaped decision boundaries which is more flexible than classifiers constrained to rectilinear decision boundaries: (DT/LinearSVC )</h4>
<p>Cross-Validation was set to 6 for each model, meaning that we train on 5 separate training sets and test on the remaining set(that the model has never seen before) for each fold. This helps us minimize the cross validation error and helps us in finding the optimal number of k-neighbors in order to maximize recall.<br />
Cross validation error: allows us to estimate how accurately the model would perform if the data we collected is an accurate representation of the real world.</p>
<figure>
    <img src="https://i.ibb.co/WFbBd2n/Screen-Shot-2020-12-19-at-12-56-12-AM.png"/> <figcaption>
            <h4>KNN ROC AUC</h4>
        </figcaption>
</figure>

<figure>
    <img src="https://i.ibb.co/PrXbGQX/Screen-Shot-2020-12-19-at-12-58-31-AM.png"/> <figcaption>
            <h4>Optimized KNN Metrics</h4>
        </figcaption>
</figure>

<h2 id="support-vector-classifier-linear">Support Vector Classifier (Linear)</h2>
<h4 id="pros-and-cons">Pros and Cons:</h4>
<p>SVC tries to find the best hyperplane to separate different classes: maximizes distance between sample point and hyperplane. Distance-based algorithm so MinMaxScaler() data was re-used</p>
<h5 id="pros">Pros:</h5>
<ul>
<li>can deal with both linear and nonlinear decision boundaries</li>
<li>Robust to noisy data</li>
</ul>
<h5 id="cons">Cons:</h5>
<ul>
<li>Lack of interpretability. Black-box model<br />
<figure>
    <img src="https://i.ibb.co/qRwTWMD/Screen-Shot-2020-12-19-at-1-01-07-AM.png"/> <figcaption>
            <h4>Linear SVC ROC AUC</h4>
        </figcaption>
</figure>
</li>
</ul>
<figure>
    <img src="https://i.ibb.co/c6qRPDK/Screen-Shot-2020-12-19-at-1-02-35-AM.png"/> <figcaption>
            <h4>Grid Searched Linear SVC Metrics</h4>
        </figcaption>
</figure>

<h4 id="svm-radial-basis-function">SVM-Radial Basis Function</h4>
<ul>
<li>RBF is a Kernel that assumes a Gaussian distribution for each attribute. It works by finding the distance between data points in a higher dimensional(squared) space than Linear SVC. Due to this, it is extremely time-consuming and computationally expensive to Grid Search</li>
<li>gamma: learning-rate for non linear hyperplanes. Contols the tradeoff between error due to bias and variance</li>
<li>Higher gamma tries to exactly fit the training data set which is why we want to be careful in terms of overfitting</li>
<li>moves in the same direction as C</li>
</ul>
<h3 id="svc-rbf-roc-auc">SVC RBF ROC AUC</h3>
<figure>
    <img src="https://i.ibb.co/0Xzs4nW/Screen-Shot-2020-12-19-at-1-04-15-AM.png"/> 
</figure>

<h3 id="svc-rbf-optimized-metrics">SVC RBF Optimized Metrics</h3>
<figure>
    <img src="https://i.ibb.co/qdB3NDh/Screen-Shot-2020-12-19-at-1-06-02-AM.png"/> 
</figure>

<h3 id="svc-key-take-aways">SVC Key Take-aways:</h3>
<p>SVC-RBF performed no better than LinearSVC() and it consumed a maority of time to GridSearch and Evaluate. Perhaps for this problem, mapping each poinnt into a higher dimensional space isn&rsquo;t completely necessary.<br />
following GridSearch our C value for SVC-RBF was markedly higher(2) than the optimized C value for our linear SVC(0.5). A Higher C almost always leads to higher accuracy on our training data but could compromise accuracy on our testing data if it is not perfectly representative of our training data.<br />
This is why cross validation. If we were to use a Medium-C (between 0.5 and 2 for our two Support vector classifiers) then we give up the ability to get perfect training accuracy but we end up with a larger margin (between the two classes in return), leading to a more generalizeable model. So training accuracy may be lowered, but if the testing data is accurately representative then it could have a higher testing accuracy overall.</p>
<p>In the linear SVC we have the lowest C which maximizes the margin between each point. Given outliers(continuous numeric variables) were winsorized and then imputed using MICE, we will end up with lower variance overall. Additionally, scaling our data(MinMaxScaler()) transforms features on a scale from 0 to 1, which places all points within a closer proximity. Keep in mind outliers were adequately dealt with in our initial preprocessing, but min max scaler was useful in every model following NB. Even with a high and low C in both of our SVC models, the training and testing accuracy was quite similar which means it was neither overfitting nor underfitting. For our Decision Boundary(SVC-RBF) there is no difference between&rsquo;ovo' and the default &lsquo;ovr&rsquo; parameter. This is because we are simply dealing with a binary classification problem instead of multi-class. If we were dealing with numerous classes I would&rsquo;ve elected to use &lsquo;one verse rest&rsquo; in order to achieve optimal decision boundaries between each class(classic with iris dataset). &lsquo;One Versus Rest&rsquo; consists of fitting one classifier per class and one versus one would achieve the exact same thing.</p>
<p>Since we have an equal number of positive and negative cases in our training dataset, there was no need to set the class_weight = &lsquo;balanced&rsquo;. If postive to negative cases were 1:10, for example than updating our Cost parameter to class weight (0.1)*C for SVC. Since this isn&rsquo;t given each class is weighted one automatically.</p>
<p>random_stateint, RandomState instance or None, optional (default=None)<br />
The seed of the pseudo random number generator to use when shuffling the data for the dual coordinate descent (if dual=True).</p>
<p>For dual was set to false which is preferred when we have far more samples than features. Since dual is equal to False for LinearSVC, random State doesn&rsquo;t have any effect on the model.</p>
<p>LinearSVC: penalty function was set to &lsquo;l1&rsquo; because after scaling, this dataset behaved almost sparsely which means that the cross validation error had the strongest correlation to testing error with this function, which is preferred on scaled function. &lsquo;l1&rsquo; can help our function optimally adjust C to account for different # training samples<br />
&lsquo;Squared-hinge&rsquo; was selected</p>
<h1 id="randomforestclassifier">RandomForestClassifier:</h1>
<ul>
<li>Ensemble method that takes a subset of observations and a subset of variables to build a decision trees.</li>
<li>builds multiple DT&rsquo;s and combines the best ones based on maximum votign from a panel of independent(unbiased judges)</li>
</ul>
<h2 id="feature-importance-of-random-forest-attributes">Feature Importance of Random Forest Attributes</h2>
<figure>
    <img src="https://i.ibb.co/vwnqcFF/Screen-Shot-2020-12-17-at-12-14-15-AM.png"/> 
</figure>

<h2 id="optimized-random-forest-roc-auc">Optimized Random Forest ROC-AUC</h2>
<figure>
    <img src="https://i.ibb.co/3rjkbvv/Screen-Shot-2020-12-19-at-1-08-37-AM.png"/> 
</figure>

<p>Overall, each model performed extremely similarly in terms of precision, recall and accuracy.<br />
This tells me that the data preprocessing steps definitely had the highest impact on model performance. With regards to compromise inn disease modeling it is almost always better to be cautious and slightly over-diagnose. It&rsquo;s okay if more false positives are thrown, but if a patient has this disease and the model tells them they are fine (FN) then recall decreases and this is largely problematic. I attribute this to a few different things.</p>
<h1 id="logistic-regression-classifier-and-artificial-neural-networks">Logistic Regression Classifier and Artificial Neural Networks</h1>
<h3 id="logistic-regression-modeling">Logistic Regression Modeling:</h3>
<h4 id="note-the-goal-for-each-supervised-learning-model-is-to-maximize-recall">NOTE: The goal for each supervised learning model is to maximize recall</h4>
<p>We would like to maximize recall in this context because it is better to predict that a patient has this disease when they&rsquo;re healthy than to throw a FalseNegative when they indeed do have it. This could lead to people missing the critical time window for proper care and should be considered a life or death situation. That being said, we don&rsquo;t want to completely compromise accuracy just for the sake of recall; i.e.. A really bad model could predict that the patient has the disease 100% of the time for example.</p>
<p>In the case of ANN&rsquo;s we will only be tuning the model on mminimum values from the binary cross entropy loss functions, which are paired with model accuracy. These models(MLPs)tend to show more balanced results across each metric which is why the ANN predictions will still be ranked by order of recall.</p>
<h3 id="scalingsplitting-the-data">Scaling/Splitting the data:</h3>
<h4 id="note">NOTE:</h4>
<ul>
<li>Scaling the data did not have much of an impact on model performance however it did boost recall by ~0.5 % which is why it was passed through our Logistic Regression Classifier.</li>
<li>I believe this has to do with how adequately we dealt with outliers in the preprocessing step; so the default &lsquo;l2&rsquo; penalty function didn&rsquo;t yield any major metric changes before/after scaling.</li>
<li>The only remaining attributes that contain outliers (above the whisker) are height and weight, which are plausible(especially in the United States), thus these demand slightly more emphasis when creating an efficient classifier. One method to deal with this collinearity in our shallow learning algorithms would be to create a separate columnn such as BMI which  would combine height and weight into a new feature that would tell us more about each. observation and reduce model complexity(fewer attributes).</li>
</ul>
<h3 id="optimized-logistic-regression-roc-auc">Optimized Logistic Regression ROC AUC</h3>
<figure>
    <img src="https://i.ibb.co/LhVVgbD/Screen-Shot-2020-12-19-at-1-31-23-AM.png"/> 
</figure>

<h1 id="ann-modeling">ANN Modeling:</h1>
<p>Now we will explore Artificial Neural Network&rsquo;s with up to 2 hidden layers. Note that each model has been GridSearched  in google collab in order to obtain the satisfactory hyperparameters.</p>
<ul>
<li>ANN&rsquo;s can handle multicollinearity so things such as Height and Weight won&rsquo;t stifle our model. This is one of the advantages of deep learning over some of the previous shallow algoprithms we&rsquo;ve used.</li>
<li>Standard Scalar was applied to avoid dying nodes based on certain weights associated with our features. This means the rest of the network won&rsquo;t be offset by different metrics/units of measurement for each attribute.</li>
<li>binary probabilistic distribution means outputs must add to 1 and that the results are mutually exclusive</li>
</ul>
<h2 id="model-tuning">MODEL Tuning:</h2>
<h3 id="model-hyper-parameters-to-tune-in-deep-learning">Model Hyper-Parameters to Tune in Deep Learning</h3>
<ul>
<li>we started off with 0 hidden layers meaning that our 11 features were squished into one node between 0 and 1 using a sigmoid function.</li>
<li>Loss function: Set to Binary_crossentropy since we are essentially dealing with a probability distribution for each predicted class.</li>
<li>Activation function: rectified linear unit for hidden layers and sigmoid for output layers.</li>
<li>Rectified Linear Units were used in the hidden layers because they helped reduce the likelihood of a vaninshing gradient (occurs with sigmoids becominng increasingly smaller over a training time)</li>
<li>Sparsity is also a huge benifit of ReLu (hidden layers) because any activation output &lt; 0 is automatically set to 0. This is opposed to sigmoid activation functions which are almost always going to produce some sort of non zero value between 0 and 1.</li>
<li>Optimization method: Adam was chosen for each method: it&rsquo;s classical stochastic gradient descent proved to be just as efficient as SGD and other methods. Additionally, it&rsquo;s straightforward implementation and wide-use across industry, made it suitable for our classification task.</li>
<li>Neural nets can face the problem of plateuing at times. A Majority of this issue can be solved usinng the EarlyStoppingCallBack from the keras package. Patience = 15 means the model waits 15 epochs to check if val_loss decreases further before stopping the training process. This method was used as a means of guaging out how many epochs our original model was going to need to converge on a minima</li>
</ul>
<h2 id="ann-w-0-hidden-layers-roc-auc">ANN w/ 0 Hidden Layers ROC-AUC</h2>
<figure>
    <img src="https://i.ibb.co/xjsXrJB/Screen-Shot-2020-12-19-at-1-33-29-AM.png"/> 
</figure>

<h5 id="comparing-logistic-regression-with-linear-svm-and-single-layer-perceptron">Comparing Logistic Regression with linear SVM, and single layer perceptron:</h5>
<ul>
<li>All of these models are operating on a similar principal which, in turn is leading to extremely similar results.</li>
</ul>
<p>This is in part due to the fact that the fitted logistic regression is a sigmoid curve   ( output layer : ANN0) representing the probability of a person having the disease given those 11 features. The probability distributions for each datapoint is mapped out and  then, via backpropahation, penalized by taking the and negative log of the probability which introduces our idea of a loss function</p>
<ul>
<li>
<p>The early stopping technique helped us in determining where loss of the model was minimized. The patience of 10 epochs allowed the model to keep exploring the training data to determine if the loss of the model could change further(local minima).</p>
</li>
<li>
<p>This model seemed to bottom out at 0.5535 loss on the training data and 0.5545 on the validation loss indicating that this  fit nicely into our validation data. If our val_loss was less than our loss, this would be an issue.</p>
</li>
<li>
<p>As the probability of the actual true class gets closer to 0, the -log of loss increases exponentially; thus the mean of all these losses is representative of the label&rsquo;s distribution in the data itself which is why I think these three models performed so similarly. The batch size of 100 was manually changed, periodically throughout this experiment however batch sizes that were too small &lt; 10 would take much longer to be passed through our network.</p>
</li>
<li>
<p>In the LR we have the lowest C which maximizes the margin between each point and it&rsquo;s hyperplane. Given  that outliers(continuous numeric variables) were winsorized and then imputed using MICE, we will end up with lower variance overall.</p>
</li>
<li>
<p>Both linear SVM annd LR utilized a low Cost parameter in order to maximize recall</p>
</li>
</ul>
<h2 id="ann-continued-1-layer-hidden-layer">ANN Continued: 1 Layer Hidden Layer</h2>
<p><figure>
    <img src="https://i.ibb.co/LrZzhGY/Screen-Shot-2020-12-19-at-1-35-44-AM.png"/> 
</figure>
<br />
This is the best model we have seen thus far in terms of balancing recall precision and accuracy</p>
<h4 id="recall-and-accuracy-are-up-for-both-anns-that-contain-hidden-layers">Recall and Accuracy are up for both ANN&rsquo;s that contain hidden layers.</h4>
<p>With regards to hyperparamter tuning; the smaller the batch_size, the less accurate the estimate of the gradient seemed to be and the more time consuming the process was overall. However, as more layers are added the complexity is increased which is why smaller batch sizes were fed to the model. A larger number of epochs was required to converge on a minimum as well.</p>
<h1 id="master-performance-table-1">Master Performance Table</h1>
<figure>
    <img src="https://i.ibb.co/fSCyGjY/newplot-6.png"/> 
</figure>

<ul>
<li>
<p>Implemented machine learning algorithms such as Decision Trees, Support Vector Machines (SVM), Random Forest, Naïve Bayes, K-Nearest Neighbours (KNN), Logistic Regression, and Artificial Neural Networks.</p>
</li>
<li>
<p>Used Grid Search and k-fold cross-validation to tune the hyper-parameters in order to optimize the recall and accuracy of the models</p>
</li>
<li>
<p>Created disease predictions with accuracy and recall values of around 78%</p>
</li>
</ul>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://ryanondocin2020.github.io/ryan_ondocin_portfolio/" >
    &copy;  Ryan Ondocin 2021 
  </a>
    <div>







<a href="https://www.linkedin.com/in/ryan-ondocin-5699b7138/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/Ryanondocin2019?tab=repositories" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>







</div>
  </div>
</footer>

    

  <script src="https://ryanondocin2020.github.io/ryan_ondocin_portfolio/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
